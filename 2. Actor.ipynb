{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Actors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor** is a stateful(remembering the state) worker. <br>\n",
    "When a new actor is instantiated, a new worker is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-12 10:52:00,614\tWARNING worker.py:1381 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-03-12 10:52:00,616\tINFO node.py:439 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-12_10-52-00_68461/logs.\n",
      "2019-03-12 10:52:00,723\tINFO services.py:364 -- Waiting for redis server at 127.0.0.1:50769 to respond...\n",
      "2019-03-12 10:52:00,834\tINFO services.py:364 -- Waiting for redis server at 127.0.0.1:16026 to respond...\n",
      "2019-03-12 10:52:00,838\tINFO services.py:761 -- Starting Redis shard with 3.44 GB max memory.\n",
      "2019-03-12 10:52:00,848\tINFO services.py:1449 -- Starting the Plasma object store with 5.15 GB memory using /tmp.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '10.30.198.126:50769',\n",
       " 'object_store_address': '/tmp/ray/session_2019-03-12_10-52-00_68461/sockets/plasma_store',\n",
       " 'webui_url': None,\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-03-12_10-52-00_68461/sockets/raylet'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining and creating an actor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ray.remote*** indicates that Counter class will be actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "    \n",
    "    def increment(self):\n",
    "        self.value += 1\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Counter.remote()*** can actually create an actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Counter.remote()\n",
    "a2 = Counter.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using an actor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(010000003129c12980aeb228e2a486fe0a6b5694)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.increment.remote()\n",
    "a2.increment.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ***a1.increment.remote()*** is called, the following events happens. <br>\n",
    "\n",
    "1. A task is created.\n",
    "2. The task is assigned directly to the local scheduler responsible for the actor by the driverâ€™s local scheduler.\n",
    "3. An object ID is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function in the same object is scheduled on the same local scheduler = ***serial***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-12 10:44:51,125\tERROR worker.py:1752 -- WARNING: 12 workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2019-03-12 10:44:51,438\tERROR worker.py:1752 -- WARNING: 16 workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Create ten Counter actors.\n",
    "counters = [Counter.remote() for _ in range(10)]\n",
    "\n",
    "# Increment each Counter once and get the results. These tasks all happen in\n",
    "# parallel.\n",
    "results = ray.get([c.increment.remote() for c in counters])\n",
    "print(results)  # prints [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Increment the first Counter five times. These tasks are executed serially\n",
    "# and share state.\n",
    "results = ray.get([counters[0].increment.remote() for _ in range(5)])\n",
    "print(results)  # prints [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A More Interesting Actor Example - Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "@ray.remote\n",
    "class GymEnvironment(object):\n",
    "    def __init__(self, name):\n",
    "        self.env = gym.make(name)\n",
    "        self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = GymEnvironment.remote(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(01000000615a51a0120a6a6f11a4d0c116ff31dd)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pong.step.remote(0)  # Take action 0 in the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make it easy usage of parallell simulation in gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Actor with Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ray\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b46be0f6b76f>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/joonleesky/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/joonleesky/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/joonleesky/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/joonleesky/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/joonleesky/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class NeuralNet(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.loss = self.build_network()\n",
    "        self.train_op = self.add_training_op()\n",
    "        \n",
    "        self.init_network()\n",
    "        \n",
    "    def init_network(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def build_network(self):\n",
    "        self.x = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "        \n",
    "        self.W = tf.Variable(tf.zeros([784, 10]))\n",
    "        self.b = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "        score = tf.matmul(self.x, self.W) + self.b\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = self.y))\n",
    "    \n",
    "    def add_training_op(self):\n",
    "        return tf.train.GradientDescentOptimizer(0.5).minimize(self.loss)\n",
    "    \n",
    "    \n",
    "    def train(self, data, epoch, batch_size):\n",
    "        total_batch = data.train.num_examples // batch_size\n",
    "        for e in range(epoch):\n",
    "            total_loss = 0.0\n",
    "            for _ in range(total_batch):\n",
    "                batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "                _, loss_val = self.sess.run([self.train_op, self.loss], \n",
    "                                            feed_dict = {self.x: batch_xs, self.y: batch_ys})\n",
    "                total_loss += loss_val\n",
    "            print('Epoch: %04d' %(e+1), end = '  ')\n",
    "            print('Avg Loss: %.3f' %(total_loss / total_batch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m 2019-03-12 10:53:06.857009: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m 2019-03-12 10:53:06.857171: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m 2019-03-12 10:53:06.856735: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m 2019-03-12 10:53:06.856906: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m WARNING:tensorflow:From <ipython-input-4-90ce24ab9126>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m WARNING:tensorflow:From <ipython-input-4-90ce24ab9126>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "nns = [NeuralNet.remote() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Multiple Actor is training neural network seperately***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectID(0100000049327a0264ecbc7e0cf2f9eec9ad6496),\n",
       " ObjectID(01000000a57452373d13c85b7c0c1ca744247bb2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0001  Avg Loss: 0.266\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0001  Avg Loss: 0.267\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0002  Avg Loss: 0.265\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0002  Avg Loss: 0.265\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0003  Avg Loss: 0.264\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0003  Avg Loss: 0.263\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0004  Avg Loss: 0.262\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0004  Avg Loss: 0.263\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0005  Avg Loss: 0.261\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0005  Avg Loss: 0.261\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0006  Avg Loss: 0.260\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0006  Avg Loss: 0.260\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0007  Avg Loss: 0.260\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0007  Avg Loss: 0.259\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0008  Avg Loss: 0.259\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0008  Avg Loss: 0.258\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0009  Avg Loss: 0.257\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0009  Avg Loss: 0.259\n",
      "\u001b[2m\u001b[36m(pid=68601)\u001b[0m Epoch: 0010  Avg Loss: 0.257\n",
      "\u001b[2m\u001b[36m(pid=68600)\u001b[0m Epoch: 0010  Avg Loss: 0.256\n"
     ]
    }
   ],
   "source": [
    "[nn.train.remote(data=mnist, epoch = 10, batch_size = 128) for nn in nns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
